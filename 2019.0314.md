###### 1.损失函数
1. 0-1损失函数
   如果预测值和目标值相等，值为0，如果不相等，值为1.

$$
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}
$$

一般的在实际使用中，相等的条件过于严格，可适当放宽条件：
$$
L(Y, f(x)) =
\begin{cases}
1,& |Y-f(x)|\ge T\\
0,& |Y-f(x)|< T
\end{cases}
$$

2. 绝对值损失函数
   和0-1损失函数相似，绝对值损失函数表示为：

$$
L(Y, f(x)) = |Y-f(x)|​
$$

3. 平方损失函数

$$
L(Y, f(x)) = \sum_N{(Y-f(x))}^2
$$

这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该使所有点到回归直线的距离和最小。

4. log对数损失函数

$$
L(Y, P(Y|X)) = -\log{P(Y|X)}
$$

常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数式平方损失，其实不然。逻辑回归它假设样本服从伯努利分布，进而求得满足该分布的似然函数，接着取对数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看，就是log损失函数。

5. 指数损失函数
   指数损失函数的标准形式为：

$$
L(Y, f(x)) = \exp{-yf(x)}
$$

例如AdaBoost就是以指数损失函数为损失函数。

6. Hinge损失函数
   Hinge损失函数的标准形式如下：

$$
L(Y) = \max{(0, 1-ty)}
$$

其中y是预测值，范围为(-1,1),t为目标值，其为-1或1.

在线性支持向量机中，最优化问题可等价于
$$
\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w^2\Vert
$$
上式相似于下式
$$
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w^2\Vert
$$
其中前者是Hinge损失函数，后面可看做为正则化项。

###### 2.stacking和blending的区别
[img](https://img-blog.csdn.net/20170915114447314?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3N0Y2pm/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

###### 3.包含min函数的栈

 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。

```
class Solution {
public:
    void push(int value) {
        
}
void pop() {
    
}
int top() {
    
}
int min() {
    
}

};

```

